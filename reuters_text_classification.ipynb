{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "#===   change to the correct directory\n",
    "#==============================================================================\n",
    "import os\n",
    "path = r'C:\\Users\\eight\\Desktop\\Kelvin HDD\\3. Coursera\\A. Projects\\1. Reuters text classification'\n",
    "os.chdir(path)\n",
    "os.getcwd()\n",
    "\n",
    "#==============================================================================\n",
    "#===   unzip the tar.gz files\n",
    "#==============================================================================\n",
    "import tarfile\n",
    "tar = tarfile.open('reuters21578.tar.gz')\n",
    "tar.getnames()  # displays the names of the files within the tar file\n",
    "tar.extractall()  # this will unzip the files into the current working directory\n",
    "\n",
    "#==============================================================================\n",
    "#===   some basic configuration settings\n",
    "#==============================================================================\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eight\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\eight\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19305475"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#==============================================================================\n",
    "#===   use beautiful soup \n",
    "#==============================================================================\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "num = len([name for name in os.listdir() if name.endswith('sgm')])\n",
    "data = open('reut2-00{}.sgm'.format(0), 'r').read()\n",
    "for i in np.arange(1,num):\n",
    "\tif i < 10:\n",
    "\t\tadd_data = open('reut2-00{}.sgm'.format(i), 'r').read()\n",
    "\t\tdata = str(data) + str(add_data)\n",
    "\telse:\n",
    "\t\tadd_data = open('reut2-0{}.sgm'.format(i), 'r').read()\n",
    "\t\tdata = str(data) + str(add_data)\n",
    "\t\t\n",
    "\tsoup = BeautifulSoup(str(data))\n",
    "\n",
    "#data = open('reut2-00{}.sgm'.format(0), 'r').read()\n",
    "#soup = BeautifulSoup(str(data))\n",
    "\n",
    "#=== using \"find_all\" \n",
    "# we can print out the titles of the articles\n",
    "soup.find_all([\"title\"])  \n",
    "len(soup.find_all([\"title\"]))  # there are 20841 different articles in total\n",
    "\n",
    "# we can print out the titles of the articles\n",
    "soup.find_all([\"text\"])[:5]\n",
    "\n",
    "menu = []\n",
    "for meat in soup.find_all([\"text\"]):\n",
    "\tmenu.append(meat.contents)\n",
    "\n",
    "len(menu) \t \t# there are 21578 text blocks (as implied in the title)\n",
    "type(menu) \t \t# \"menu\" is a list object\n",
    "len(str(menu)) \t# there are 19305475 characters in the entire menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "#===   we clean up the text in the \"menu\"\n",
    "#==============================================================================\n",
    "#===   remove all the tags, and replace \"\\\\n\" with \"\\n\"\n",
    "import re\n",
    "\n",
    "new_menu = []  # this is a list of article items \n",
    "for i, item in enumerate(menu):\n",
    "\ttext = re.sub(r\"\\\\n\", \"\\n\", str(menu[i])) # replace \"\\\\n\" with \"\\n\"\n",
    "\ttext = re.sub(\"[<]+[/a-zA-Z]*[>]+\", \"\", text) # replace the tags with space\n",
    "\tnew_menu.append(text)\n",
    "\n",
    "# we want to clean up the document set, \"new_menu\", which we first combine the \n",
    "# objects in the new_menu first\n",
    "new_menu_str = new_menu[0]\n",
    "for i in range(1,len(new_menu)-1):\n",
    "\tnew_menu_str += new_menu[i]\n",
    "\n",
    "\n",
    "#===   clean up of the string; remove punctuations, square brackets, space characters\n",
    "#new_menu_str_cleaned = re.sub(\"[\\n\\d,.&'-<>;]\\\"+\", \" \", new_menu_str)  # removes punctuations\n",
    "#new_menu_str_cleaned = re.sub(\"\\]\\[\", \" \", new_menu_str_cleaned)   # removes square brackets\n",
    "#new_menu_str_cleaned = re.sub(\"[\\s]{2,}\", \" \", new_menu_str_cleaned)  # removes excessive space characters\n",
    "\n",
    "new_menu_str_cleaned = new_menu_str\n",
    "\n",
    "# to check the words before replacing them, i.e. whether they are standalone words\n",
    "re.findall(\"[\\w]+(?:dlrs)[\\w]+\", new_menu_str_cleaned)\n",
    "re.findall(\"[\\w]+(?:cts)[\\w]+\", new_menu_str_cleaned)\n",
    "re.findall(\"[\\w]+(?:mln)[\\w]+\", new_menu_str_cleaned)\n",
    "re.findall(\"[\\w]+(?:shrs)[\\w]+\", new_menu_str_cleaned)\n",
    "re.findall(\"[\\w]+(?:shr)[\\w]+\", new_menu_str_cleaned)\n",
    "\n",
    "#===   replace short hands to the full de-abbreviated forms\n",
    "new_menu_str_cleaned = re.sub(r\"\\bdlrs\\b\", \"dollars\", new_menu_str_cleaned, flags = re.I)\n",
    "new_menu_str_cleaned = re.sub(r\"\\bcts\\b\", \"cents\", new_menu_str_cleaned, flags = re.I)\n",
    "new_menu_str_cleaned = re.sub(r\"\\bmln\\b\", \"million\", new_menu_str_cleaned, flags = re.I)\n",
    "new_menu_str_cleaned = re.sub(r\"\\bavgsmlns\\b\", \"average millions\", new_menu_str_cleaned, flags = re.I)\n",
    "new_menu_str_cleaned = re.sub(r\"\\b50mlndlr\\b\", \"50 million dollars\", new_menu_str_cleaned, flags = re.I)\n",
    "new_menu_str_cleaned = re.sub(r\"\\bshr\\b\", \"shares\", new_menu_str_cleaned, flags = re.I)\n",
    "new_menu_str_cleaned = re.sub(r\"\\blbs\\b\", \"pounds\", new_menu_str_cleaned, flags = re.I)\n",
    "new_menu_str_cleaned = re.sub(r\"\\breuter\", \"\", new_menu_str_cleaned, flags = re.I)\n",
    "new_menu_str_cleaned = re.sub(\"[,.]\", \"\", new_menu_str_cleaned)  # removes separator for currency; not sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "#===   part of speech tagging\n",
    "#==============================================================================\n",
    "#===   we do these for the sentences, as part of word_tokenization as well\n",
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = sent_tokenize(new_menu_str_cleaned)\n",
    "tagged_tokens = []\n",
    "for s in sentences:\n",
    "\ttagged_tokens.append(nltk.pos_tag(word_tokenize(s)))\n",
    "\n",
    "tagged_tokens\n",
    "len(tagged_tokens)\n",
    "\n",
    "\n",
    "#===   clean up the tags\n",
    "# create a function to convert the tags, and to create \"None\" types if there is no match\n",
    "def convert_tag(tag):\n",
    "    \"\"\"Convert the tag given by nltk.pos_tag to the tag used by wordnet.synsets\"\"\"\n",
    "    \n",
    "    tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}   # missing are: \"DT\", \"IN\", \".\"\n",
    "    try:\n",
    "        return tag_dict[tag[0]]\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "cleaned_tokens = []\n",
    "for i in range(len(tagged_tokens)-1):\n",
    "\tfor j in range(len(tagged_tokens[i])-1):\n",
    "\t\tif len(tagged_tokens[i][j][0]) > 2 and convert_tag(tagged_tokens[i][j][1]) != None:\n",
    "\t\t\tcleaned_tokens.append(tagged_tokens[i][j])\n",
    "\n",
    "cleaned_tokens\n",
    "\n",
    "# =============================================================================\n",
    "# #===   clean up strings of punctuations and (excessive) space characters\n",
    "# new_menu_str_cleaned = re.sub(\"[^a-zA-Z0-9 ]\", \" \", new_menu_str_cleaned)  # removes punctuations\n",
    "# new_menu_str_cleaned = re.sub(\"[\\s]{2,}\", \" \", new_menu_str_cleaned)  # removes excessive space characters\n",
    "# =============================================================================\n",
    "\n",
    "#===   clean up the dates in the string\n",
    "jan_month_str = \"[\\d]?[\\s]?[jJ]an[\\w]*[,-]?[\\s]?[\\d]?\"\n",
    "feb_month_str = \"[\\d]?[\\s]?[fF]eb[\\w]*[,-]?[\\s]?[\\d]?\"\n",
    "mar_month_str = \"[\\d]?[\\s]?[M]ar(?:ch)*[,-]?[\\s]?[\\d]?\"\n",
    "apr_month_str = \"[\\d]?[\\s]?[aA]pr[\\w]*[,-]?[\\s]?[\\d]?\"\n",
    "may_month_str = \"[\\d]?[\\s]?[M]ay[,-]?[\\s]?[\\d]?\"\n",
    "jun_month_str = \"[\\d]?[\\s]?[jJ]un[e]*[,-]?[\\s]?[\\d]?\"\n",
    "jul_month_str = \"[\\d]?[\\s]?[jJ]ul[\\w]*[,-]?[\\s]?[\\d]?\"\n",
    "aug_month_str = \"[\\d]?[\\s]?[aA]ug[\\w]*[,-]?[\\s]?[\\d]?\"\n",
    "sep_month_str = \"[\\d]?[\\s]?[sS]ep[tember]*[,-]?[\\s]?[\\d]?\"\n",
    "oct_month_str = \"[\\d]?[\\s]?[oO]ct[\\w]*[,-]?[\\s]?[\\d]?\"\n",
    "nov_month_str = \"[\\d]?[\\s]?[nN]ov[ember]*[,-]?[\\s]?[\\d]?\"\n",
    "dec_month_str = \"[\\d]?[\\s]?[dD]ec[ember]*[,-]?[\\s]?[\\d]?\"\n",
    "\n",
    "new_menu_str_cleaned = re.sub(jan_month_str, \" \", new_menu_str_cleaned, flags = re.I)\n",
    "new_menu_str_cleaned = re.sub(feb_month_str, \" \", new_menu_str_cleaned, flags = re.I)\n",
    "new_menu_str_cleaned = re.sub(mar_month_str, \" \", new_menu_str_cleaned, flags = re.I)\n",
    "new_menu_str_cleaned = re.sub(apr_month_str, \" \", new_menu_str_cleaned, flags = re.I)\n",
    "new_menu_str_cleaned = re.sub(may_month_str, \" \", new_menu_str_cleaned)\n",
    "new_menu_str_cleaned = re.sub(jun_month_str, \" \", new_menu_str_cleaned, flags = re.I)\n",
    "new_menu_str_cleaned = re.sub(jul_month_str, \" \", new_menu_str_cleaned, flags = re.I)\n",
    "new_menu_str_cleaned = re.sub(aug_month_str, \" \", new_menu_str_cleaned, flags = re.I)\n",
    "new_menu_str_cleaned = re.sub(sep_month_str, \" \", new_menu_str_cleaned, flags = re.I)\n",
    "new_menu_str_cleaned = re.sub(oct_month_str, \" \", new_menu_str_cleaned, flags = re.I)\n",
    "new_menu_str_cleaned = re.sub(nov_month_str, \" \", new_menu_str_cleaned, flags = re.I)\n",
    "new_menu_str_cleaned = re.sub(dec_month_str, \" \", new_menu_str_cleaned, flags = re.I)\n",
    "\n",
    "\n",
    "#===   remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = set(stopwords.words(\"english\")) # build the stopwords set\n",
    "\n",
    "new_menu_str_cleaned_recreated = []\n",
    "for i in range(len(cleaned_tokens)-1):\n",
    "\tif cleaned_tokens[i][0] not in eng_stopwords:\n",
    "\t\tnew_menu_str_cleaned_recreated.append(cleaned_tokens[i][0])\n",
    "\n",
    "#type(new_menu_str_cleaned_recreated)\n",
    "\n",
    "# identify the words that have at least N consecutive uppercase characters\n",
    "# =============================================================================\n",
    "# set(re.findall(\"[A-Z]{2,}\", new_menu_str_cleaned))\n",
    "# set(re.findall(\"[A-Z]{3,}\", new_menu_str_cleaned))\n",
    "# set(re.findall(\"[A-Z]{4,}\", new_menu_str_cleaned))\n",
    "# =============================================================================\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43164"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#==============================================================================\n",
    "#===   tokenization\n",
    "#==============================================================================\n",
    "#===   create the document set of word tokens, and clean up the tokens for the \"Dictionary\" function later\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "tokenizer = RegexpTokenizer('[a-zA-Z]{3,}')  # taking out words of length at least 3\n",
    "words_list = tokenizer.tokenize(str(new_menu_str_cleaned_recreated))\n",
    "words_list = [word.lower() for word in words_list]\n",
    "\n",
    "words_combined = []\n",
    "for i, word in enumerate(words_list):\n",
    "\twords_combined.append(word)\n",
    "\t\t\n",
    "len(set(words_combined))  # there are 43164 unique words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37570"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#==============================================================================\n",
    "#===   perform lemmatization \n",
    "#==============================================================================\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "WNlemma = WordNetLemmatizer()\n",
    "\n",
    "words_combined_lemmatized = []\n",
    "for text in set(words_combined):\n",
    "\t\twords_combined_lemmatized.append(WNlemma.lemmatize(text, \"v\"))\n",
    "\n",
    "len(set(words_combined_lemmatized))  # after lemmatization, we get 37570 unique words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37570"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#==============================================================================\n",
    "#===   perform stemming to get root words\n",
    "#==============================================================================\n",
    "word_vector = words_combined_lemmatized\n",
    "\n",
    "# prepare to check against the correctly spelled words\n",
    "from nltk.corpus import words\n",
    "correct_spellings = words.words()\n",
    "\n",
    "words_filtered = []\n",
    "for text in word_vector:\n",
    "\twords_filtered.append(text)\n",
    "\n",
    "len(set(words_filtered))  # we have gotten 37570 words (if lemmatize only, else 2657), out of the total number of 83309 words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #==============================================================================\n",
    "# #===   Perform LDA topic modelling\n",
    "# #==============================================================================\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# remove tokens that don't appear in at least 1 document\n",
    "# remove tokens that appear in more than 20% of documents\n",
    "vect = CountVectorizer(min_df = 5, max_df = 0.2, stop_words = 'english')\n",
    "\n",
    "# fit and transform\n",
    "vect.fit(words_filtered)\n",
    "X = vect.transform(words_filtered)\n",
    "\n",
    "# convert sparse matrix to gensim corpus\n",
    "corpus = gensim.matutils.Sparse2Corpus(X, documents_columns = False)\n",
    "\n",
    "# mapping from word IDs to words\n",
    "id_map = dict((v,k) for k, v in vect.vocabulary_.items())\n",
    "\n",
    "# initialise the LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 10, id2word = id_map,\n",
    "\t\t\t\t\t\t\t\t\t\t   passes = 25, random_state = 34)\n",
    "\n",
    "# print out the LDA topics\n",
    "ldamodel.print_topics(num_topics = 10, num_words = 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "#===   Perform text categorisation\n",
    "#==============================================================================\n",
    "# take a look at a new document\n",
    "new_doc = [\"\\n\\nIt's my understanding that the freezing will start to occur because \\\n",
    "of the\\ngrowing distance of Pluto and Charon from the Sun, due to it's\\nelliptical orbit. \\\n",
    "It is not due to shadowing effects. \\n\\n\\nPluto can shadow Charon, and vice-versa.\\n\\nGeorge \\\n",
    "Krumins\\n-- \"]\n",
    "\n",
    "    \n",
    "X_test_vectorized = vect.transform(new_doc)\n",
    "corpus_test = gensim.matutils.Sparse2Corpus(X_test_vectorized, documents_columns = False)    \n",
    "list(ldamodel.get_document_topics(corpus_test))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #==============================================================================\n",
    "# #===   Perform LDA using bag of words\n",
    "# #==============================================================================\t\t\n",
    "# =============================================================================\n",
    "# from gensim import corpora, models\n",
    "# dictionary = corpora.Dictionary([words_filtered])  # create dictionary\n",
    "# bow_corpus = [dictionary.doc2bow(doc) for doc in [words_filtered]] # create the corpus / bag-of-words representation\n",
    "# \n",
    "# ldamodel_bow = gensim.models.LdaMulticore(bow_corpus, num_topics = 10, \n",
    "# \t\t\t\t\t\t\t\t\t  id2word = dictionary, passes = 2, workers = 2)\n",
    "# \n",
    "# ldamodel_bow.print_topics(num_topics = 5, num_words = 10)\n",
    "# \n",
    "# for idx, topic in ldamodel_bow.print_topics(-1):\n",
    "# \tprint('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "# =============================================================================\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
